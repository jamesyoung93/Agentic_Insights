import os
import json
from datetime import datetime
from typing import Dict, List, Any

class EnhancedReportGenerator:
    """
    Generates comprehensive research reports with:
    - Code transparency
    - Visualizations
    - Statistical rigor
    - Causal inference discussion
    - Confounders and limitations
    """
    
    def __init__(self, world_model, output_dir='outputs'):
        self.world_model = world_model
        self.output_dir = output_dir
    
    def generate_full_report(self) -> str:
        """Generate comprehensive report with all evidence"""
        
        report = self._header()
        report += self._executive_summary()
        report += self._methodology()
        report += self._discoveries_detailed()
        report += self._statistical_appendix()
        report += self._code_appendix()
        report += self._limitations()
        
        return report
    
    def _header(self) -> str:
        return f"""
{'='*100}
AUTONOMOUS DISCOVERY REPORT
{'='*100}
Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
Cycles Completed: {self.world_model.current_cycle}
Total Discoveries: {len(self.world_model.discoveries)}
Analyses Performed: {len(self.world_model.analyses)}
Papers Reviewed: {len(self.world_model.literature_findings)}
{'='*100}

"""
    
    def _executive_summary(self) -> str:
        summary = "\nEXECUTIVE SUMMARY\n" + "-"*100 + "\n\n"
        
        for i, disc in enumerate(self.world_model.discoveries[:5], 1):
            summary += f"{i}. {disc.get('title', 'Discovery')}\n"
            summary += f"   {disc.get('description', '')[:200]}\n\n"
        
        return summary
    
    def _methodology(self) -> str:
        method = "\nMETHODOLOGY\n" + "-"*100 + "\n\n"
        
        method += """
This report was generated through an autonomous discovery process involving:

1. ITERATIVE RESEARCH CYCLES
   - Each cycle: question generation → data analysis → literature search → synthesis
   - World model tracks cumulative knowledge across cycles
   
2. DATA ANALYSIS APPROACH
   - Python code generated by LLM for each research question
   - Statistical tests applied: t-tests, correlations, regressions, ANOVA
   - Significance threshold: p < 0.05
   - Effect sizes reported (Cohen's d, r, R²)
   
3. LITERATURE SYNTHESIS
   - Keyword-based paper retrieval
   - Cross-referencing with data findings
   - Consensus identification
   
4. CAUSAL INFERENCE CONSIDERATIONS
   - Temporal ordering assessed where possible
   - Confounders identified and discussed
   - Alternative explanations considered
   - Limitations explicitly stated

NOTE: All findings are CORRELATIONAL unless explicitly stated otherwise.
Causal claims require experimental manipulation or rigorous quasi-experimental design.

"""
        return method
    
    def _discoveries_detailed(self) -> str:
        section = "\nDETAILED DISCOVERIES\n" + "="*100 + "\n"
        
        for i, disc in enumerate(self.world_model.discoveries, 1):
            section += f"\n{'-'*100}\n"
            section += f"DISCOVERY {i}: {disc.get('title', 'Untitled')}\n"
            section += f"{'-'*100}\n\n"
            
            # Description
            section += f"FINDING:\n{disc.get('description', 'No description')}\n\n"
            
            # Statistical evidence
            if disc.get('statistical_support'):
                section += f"STATISTICAL EVIDENCE:\n{disc['statistical_support']}\n\n"
            
            # Code used
            section += "CODE USED:\n"
            if disc.get('supporting_analyses'):
                for analysis_id in disc['supporting_analyses'][:2]:
                    code_path = f"{self.output_dir}/analyses/{analysis_id}.py"
                    if os.path.exists(code_path):
                        section += f"  → {code_path}\n"
                section += "\n"
            else:
                section += "  (No code available)\n\n"
            
            # Literature support
            if disc.get('supporting_literature'):
                section += f"LITERATURE SUPPORT:\n"
                section += f"  {len(disc['supporting_literature'])} papers support this finding\n\n"
            
            # Causal inference discussion
            section += self._causal_discussion(disc)
            
            # Visualizations
            section += "VISUALIZATIONS:\n"
            viz_path = f"{self.output_dir}/analyses/viz_{i}.png"
            if os.path.exists(viz_path):
                section += f"  → {viz_path}\n\n"
            else:
                section += "  (No visualization generated)\n\n"
        
        return section
    
    def _causal_discussion(self, discovery: Dict) -> str:
        """Add causal inference discussion for each discovery"""
        
        disc = "CAUSAL INFERENCE ASSESSMENT:\n"
        disc += "┌─────────────────────────────────────────────────────────┐\n"
        
        # Assess claim type
        title = discovery.get('title', '').lower()
        desc = discovery.get('description', '').lower()
        
        if 'correlation' in title or 'correlation' in desc:
            disc += "│ Type: CORRELATIONAL                                    │\n"
            disc += "│ Causal claim: NO                                       │\n"
        elif 'impact' in title or 'effect' in title or 'causes' in title:
            disc += "│ Type: POTENTIALLY CAUSAL (requires verification)      │\n"
            disc += "│ Causal claim: TENTATIVE                                │\n"
        else:
            disc += "│ Type: DESCRIPTIVE                                      │\n"
            disc += "│ Causal claim: NO                                       │\n"
        
        disc += "└─────────────────────────────────────────────────────────┘\n\n"
        
        # Bradford Hill criteria assessment
        disc += "Bradford Hill Criteria for Causation:\n"
        disc += "  1. Temporal order: ⚠️  NOT VERIFIED (cross-sectional data)\n"
        disc += "  2. Strength: ⚠️  Effect size reported but context needed\n"
        disc += "  3. Dose-response: ⚠️  NOT TESTED\n"
        disc += "  4. Consistency: ⚠️  Single dataset (replication needed)\n"
        disc += "  5. Plausibility: ✓ Mechanism discussed in literature\n"
        disc += "  6. Confounders: ⚠️  NOT CONTROLLED (observational data)\n\n"
        
        disc += "Potential Confounders:\n"
        disc += "  - Customer demographics (age, income)\n"
        disc += "  - Temporal factors (seasonality, trends)\n"
        disc += "  - Geographic variation\n"
        disc += "  - Selection bias in sample\n"
        disc += "  - Measurement error\n\n"
        
        disc += "Alternative Explanations:\n"
        disc += "  - Reverse causation (Y causes X, not X causes Y)\n"
        disc += "  - Common cause (Z causes both X and Y)\n"
        disc += "  - Spurious correlation\n\n"
        
        disc += "Recommendations for Causal Inference:\n"
        disc += "  • Collect longitudinal data to establish temporal order\n"
        disc += "  • Control for confounders via regression or matching\n"
        disc += "  • Consider instrumental variables or natural experiments\n"
        disc += "  • Replicate in independent dataset\n"
        disc += "  • Consider experimental manipulation if feasible\n\n"
        
        return disc
    
    def _statistical_appendix(self) -> str:
        section = "\n" + "="*100 + "\n"
        section += "STATISTICAL APPENDIX\n"
        section += "="*100 + "\n\n"
        
        section += "All analyses conducted using Python with:\n"
        section += "  - pandas (data manipulation)\n"
        section += "  - scipy.stats (statistical tests)\n"
        section += "  - statsmodels (regression, ANOVA)\n"
        section += "  - scikit-learn (machine learning)\n\n"
        
        section += "Statistical Significance:\n"
        section += "  - α = 0.05 (5% false positive rate)\n"
        section += "  - Two-tailed tests unless specified\n"
        section += "  - Multiple comparison correction: NOT APPLIED (limitation)\n\n"
        
        section += "Effect Size Interpretation:\n"
        section += "  Cohen's d: |0.2| small, |0.5| medium, |0.8| large\n"
        section += "  Pearson r: |0.1| small, |0.3| medium, |0.5| large\n"
        section += "  R²: 0.01 small, 0.09 medium, 0.25 large\n\n"
        
        return section
    
    def _code_appendix(self) -> str:
        section = "\n" + "="*100 + "\n"
        section += "CODE APPENDIX\n"
        section += "="*100 + "\n\n"
        
        section += "All analysis code is available in: outputs/analyses/\n\n"
        
        for i, analysis in enumerate(self.world_model.analyses[:10], 1):
            section += f"{i}. {analysis.get('question', 'Unknown')[:80]}\n"
            if analysis.get('analysis_id'):
                section += f"   Code: {self.output_dir}/analyses/{analysis['analysis_id']}.py\n"
            section += "\n"
        
        if len(self.world_model.analyses) > 10:
            section += f"... and {len(self.world_model.analyses) - 10} more analyses\n\n"
        
        return section
    
    def _limitations(self) -> str:
        section = "\n" + "="*100 + "\n"
        section += "LIMITATIONS & CAVEATS\n"
        section += "="*100 + "\n\n"
        
        section += """
1. DATA LIMITATIONS
   • Cross-sectional data (cannot establish temporal causation)
   • Potential selection bias in sample
   • Missing data may introduce bias
   • Self-reported satisfaction may have measurement error
   
2. STATISTICAL LIMITATIONS
   • Multiple comparisons increase false positive risk
   • Assumptions of statistical tests may be violated
   • Effect sizes sensitive to outliers
   • Confounders not fully controlled
   
3. METHODOLOGICAL LIMITATIONS
   • Automated code generation may have errors
   • LLM interpretations require expert verification
   • Literature search limited to provided corpus
   • No experimental manipulation (all observational)
   
4. GENERALIZABILITY
   • Findings specific to this dataset
   • May not generalize to other contexts
   • Replication needed for validation
   
5. CAUSAL INFERENCE
   • Most findings are CORRELATIONAL only
   • Causal claims require:
     - Temporal precedence (X before Y)
     - Covariation (X and Y related)
     - No alternative explanations
   • Use findings as hypotheses for future research

RECOMMENDATION: Treat all discoveries as hypothesis-generating.
Validate with independent data and/or experimental design before acting.

"""
        
        return section
    
    def save_report(self, filename='enhanced_report.txt'):
        """Save comprehensive report"""
        report = self.generate_full_report()
        filepath = os.path.join(self.output_dir, filename)
        
        with open(filepath, 'w') as f:
            f.write(report)
        
        print(f"\n✓ Enhanced report saved: {filepath}")
        return filepath
    
    def generate_jupyter_notebook(self):
        """Generate Jupyter notebook with all analyses for reproducibility"""
        
        notebook = {
            "cells": [
                {
                    "cell_type": "markdown",
                    "metadata": {},
                    "source": ["# Autonomous Discovery Report\n", 
                              f"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n",
                              "This notebook contains all analyses performed during the discovery process."]
                },
                {
                    "cell_type": "code",
                    "metadata": {},
                    "source": ["import pandas as pd\n",
                              "import numpy as np\n",
                              "import matplotlib.pyplot as plt\n",
                              "import seaborn as sns\n",
                              "from scipy import stats\n",
                              "%matplotlib inline"]
                }
            ],
            "metadata": {
                "kernelspec": {
                    "display_name": "Python 3",
                    "language": "python",
                    "name": "python3"
                }
            },
            "nbformat": 4,
            "nbformat_minor": 4
        }
        
        # Add each analysis as a cell
        for i, analysis in enumerate(self.world_model.analyses, 1):
            # Markdown header
            notebook["cells"].append({
                "cell_type": "markdown",
                "metadata": {},
                "source": [f"## Analysis {i}: {analysis.get('question', 'Unknown')}"]
            })
            
            # Code cell
            code_path = f"{self.output_dir}/analyses/{analysis.get('analysis_id')}.py"
            if os.path.exists(code_path):
                with open(code_path, 'r') as f:
                    code = f.read()
                
                notebook["cells"].append({
                    "cell_type": "code",
                    "metadata": {},
                    "source": code.split('\n')
                })
        
        # Save notebook
        nb_path = os.path.join(self.output_dir, 'discovery_analysis.ipynb')
        with open(nb_path, 'w') as f:
            json.dump(notebook, f, indent=2)
        
        print(f"\n✓ Jupyter notebook saved: {nb_path}")
        return nb_path
